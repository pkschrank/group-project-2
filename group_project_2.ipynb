{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa274e8",
   "metadata": {},
   "source": [
    "# S&P 500 Signal Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeec5d3e",
   "metadata": {},
   "source": [
    "Approach:\n",
    "1. Create features\n",
    "2. Winsorize data\n",
    "3. Test, Train split data\n",
    "4. Scale the test and train data\n",
    "6. Original, Oversample, and Undersample the data\n",
    "7. Apply RandomForestClassifier, XGBoost, and LightBoost to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be1bde",
   "metadata": {},
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91feaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Yahoo Finance\n",
    "!pip install yfinance --upgrade --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ef0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SciKit Learn\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33ebf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install XGBoost\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15948f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LightBoost\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd667914",
   "metadata": {},
   "source": [
    "## Import Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92216ef8-6013-4ff6-b1f7-0d91479ef60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries and dependencies\n",
    "import yfinance as yf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import balanced_accuracy_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a3c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c069a5e",
   "metadata": {},
   "source": [
    "## Retrieve and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce0fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive data\n",
    "sp500 = yf.Ticker('^GSPC')\n",
    "sp500 = sp500.history(period='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b35a7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop timezone from datetime\n",
    "sp500 = sp500.reset_index()\n",
    "sp500['Date'] = sp500['Date'].dt.tz_localize(None)\n",
    "sp500.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba059b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last rows of the DataFrame\n",
    "sp500 = sp500.tail(24252) # Get max available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e97f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data\n",
    "sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c2c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stock df\n",
    "stock_df = pd.DataFrame(sp500).dropna()\n",
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2952a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "stock_df.drop(columns={'Volume', 'Dividends', 'Stock Splits'}, inplace=True)\n",
    "\n",
    "# Sort by ascending date\n",
    "stock_df = stock_df.sort_values(by=\"Date\", ascending=True)\n",
    "\n",
    "# Review the first and last five rows of the DataFrame\n",
    "display(stock_df.head())\n",
    "display(stock_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c736d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename dataframe to data\n",
    "data = stock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d3d293",
   "metadata": {},
   "source": [
    "## Perform Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d2a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Moving Averages\n",
    "data['SMA10'] = data['Close'].rolling(window=10).mean()\n",
    "data['SMA20'] = data['Close'].rolling(window=20).mean()\n",
    "data['SMA30'] = data['Close'].rolling(window=20).mean()\n",
    "data['SMA50'] = data['Close'].rolling(window=50).mean()\n",
    "data['SMA100'] = data['Close'].rolling(window=100).mean()\n",
    "data['SMA200'] = data['Close'].rolling(window=200).mean()\n",
    "\n",
    "# RSI\n",
    "def rsi(data, window=14):\n",
    "    delta = data['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "data['RSI'] = rsi(data)\n",
    "\n",
    "# MACD\n",
    "data['EMA12'] = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "data['EMA26'] = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = data['EMA12'] - data['EMA26']\n",
    "data['SL'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# Bollinger Bands\n",
    "data['BollM'] = data['Close'].rolling(window=20).mean()\n",
    "data['BollU'] = data['BollM'] + (2 * data['Close'].rolling(window=20).std())\n",
    "data['BollL'] = data['BollM'] - (2 * data['Close'].rolling(window=20).std())\n",
    "\n",
    "# Identify crossover signal for Target Column\n",
    "# Close < Bollinger Mid and RSI < 50 and MACD < 0\n",
    "data['Signal'] = np.where((data['Close'] < data['BollM']) & (data['RSI'] < 50) & (data['MACD'] < 0), 1, 0)\n",
    "\n",
    "# Calculate return\n",
    "data['Return'] = data['Close'].pct_change().shift(-1)  # Next day's return\n",
    "\n",
    "# Label Data\n",
    "# Buy ONLY if all Crossover conditions are met\n",
    "# Assume profitable if return > 0%\n",
    "data['Target'] = np.where((data['Signal'] > 0) & (data['Return'] > 0.0), 1, 0)\n",
    "\n",
    "# Prepare dataset\n",
    "features = ['SMA10', 'SMA20', 'SMA30', 'SMA50', 'SMA100', 'SMA200', 'Signal', 'RSI', 'MACD', 'SL', 'BollM', 'BollU', 'BollL']\n",
    "dataset = data.dropna()[features + ['Target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d82e39",
   "metadata": {},
   "source": [
    "## Perform Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78dbc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Moving Averages\n",
    "data['SMA10'] = data['Close'].rolling(window=10).mean()\n",
    "data['SMA20'] = data['Close'].rolling(window=20).mean()\n",
    "data['SMA30'] = data['Close'].rolling(window=20).mean()\n",
    "data['SMA50'] = data['Close'].rolling(window=50).mean()\n",
    "data['SMA100'] = data['Close'].rolling(window=100).mean()\n",
    "data['SMA200'] = data['Close'].rolling(window=200).mean()\n",
    "\n",
    "# RSI\n",
    "def rsi(data, window=14):\n",
    "    delta = data['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "data['RSI'] = rsi(data)\n",
    "\n",
    "# MACD\n",
    "data['EMA12'] = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "data['EMA26'] = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = data['EMA12'] - data['EMA26']\n",
    "data['SL'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# Bollinger Bands\n",
    "data['BollM'] = data['Close'].rolling(window=20).mean()\n",
    "data['BollU'] = data['BollM'] + (2 * data['Close'].rolling(window=20).std())\n",
    "data['BollL'] = data['BollM'] - (2 * data['Close'].rolling(window=20).std())\n",
    "\n",
    "# Identify crossover signal for Target Column\n",
    "# Close < Bollinger Mid and RSI < 50 and MACD < 0\n",
    "data['Signal'] = np.where((data['Close'] < data['BollM']) & (data['RSI'] < 50) & (data['MACD'] < 0), 1, 0)\n",
    "\n",
    "# Calculate return\n",
    "data['Return'] = data['Close'].pct_change().shift(-1)  # Next day's return\n",
    "\n",
    "# Label Data\n",
    "# Buy ONLY if all Crossover conditions are met\n",
    "# Assume profitable if return > 0%\n",
    "data['Target'] = np.where((data['Signal'] > 0) & (data['Return'] > 0.0), 1, 0)\n",
    "\n",
    "# Prepare dataset\n",
    "features = ['SMA10', 'SMA20', 'SMA30', 'SMA50', 'SMA100', 'SMA200', 'Signal', 'RSI', 'MACD', 'SL', 'BollM', 'BollU', 'BollL']\n",
    "dataset = data.dropna()[features + ['Target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24745b1a",
   "metadata": {},
   "source": [
    "## Perform Winsorization of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec523fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "data['Open'] = winsorize(np.array(data['Open']), limits=[.05, .05])\n",
    "data['High'] = winsorize(np.array(data['High']), limits=[.05, .05])\n",
    "data['Low'] = winsorize(np.array(data['Low']), limits=[.05, .05])\n",
    "data['Close'] = winsorize(np.array(data['Close']), limits=[.05, .05])\n",
    "data['SMA10'] = winsorize(np.array(data['SMA10']), limits=[.05, .05])\n",
    "data['SMA20'] = winsorize(np.array(data['SMA20']), limits=[.05, .05])\n",
    "data['SMA30'] = winsorize(np.array(data['SMA30']), limits=[.05, .05])\n",
    "data['SMA50'] = winsorize(np.array(data['SMA50']), limits=[.05, .05])\n",
    "data['SMA100'] = winsorize(np.array(data['SMA100']), limits=[.05, .05])\n",
    "data['SMA200'] = winsorize(np.array(data['SMA200']), limits=[.05, .05])\n",
    "data['EMA12'] = winsorize(np.array(data['EMA12']), limits=[.05, .05])\n",
    "data['EMA26'] = winsorize(np.array(data['EMA26']), limits=[.05, .05])\n",
    "data['MACD'] = winsorize(np.array(data['MACD']), limits=[.05, .05])\n",
    "data['BollM'] = winsorize(np.array(data['BollM']), limits=[.05, .05])\n",
    "data['BollU'] = winsorize(np.array(data['BollU']), limits=[.05, .05])\n",
    "data['BollL'] = winsorize(np.array(data['BollL']), limits=[.05, .05])\n",
    "data['SL'] = winsorize(np.array(data['SL']), limits=[.05, .05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1be0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.boxplot(data=data)\n",
    "plt.title(\"Boxplot for Outlier Detection - Train\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf625a1",
   "metadata": {},
   "source": [
    "## Perform Train and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f26b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and test sets\n",
    "X = dataset[features]\n",
    "y = dataset['Target']\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b749c53f",
   "metadata": {},
   "source": [
    "## Perform Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9196f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert scaled arrays back to DataFrames to maintain column names\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=features, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=features, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b18e33",
   "metadata": {},
   "source": [
    "# Perform Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705df91d",
   "metadata": {},
   "source": [
    "## Original Scaled Data with Untuned RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "clf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict using the model\n",
    "train_pred = clf.predict(X_train_scaled)\n",
    "test_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# Generate classification report\n",
    "# Print scores\n",
    "print(\"Balanced Accuracy Scores\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "x = balanced_accuracy_score(y_train, train_pred)\n",
    "y = balanced_accuracy_score(y_test, test_pred)\n",
    "print(balanced_accuracy_score(y_train, train_pred),'training')\n",
    "print(balanced_accuracy_score(y_test, test_pred),'testing')\n",
    "print(round((x-y), 16),'variance')\n",
    "\n",
    "# Print classification reports\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(f\"Classification Report\")\n",
    "report = classification_report(y_test, test_pred, zero_division=1)\n",
    "print(report)\n",
    "print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56dfc62",
   "metadata": {},
   "source": [
    "## Over Sampled Scaled Data with Untuned RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b58da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomOverSampler from imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Instantiate a RandomOversampler instance\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "\n",
    "# Fit the training data to the `RandomOverSampler` model\n",
    "X_oversampled, y_oversampled = ros.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Count distinct values\n",
    "y_oversampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "ros_clf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Fit the model\n",
    "ros_clf.fit(X_oversampled, y_oversampled)\n",
    "\n",
    "# Predict using the model\n",
    "train_pred = ros_clf.predict(X_train_scaled)\n",
    "test_pred = ros_clf.predict(X_test_scaled)\n",
    "\n",
    "# Generate classification report\n",
    "# Print scores\n",
    "print(\"Balanced Accuracy Scores\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "x = balanced_accuracy_score(y_train, train_pred)\n",
    "y = balanced_accuracy_score(y_test, test_pred)\n",
    "print(balanced_accuracy_score(y_train, train_pred),'training')\n",
    "print(balanced_accuracy_score(y_test, test_pred),'testing')\n",
    "print(round((x-y), 16),'variance')\n",
    "\n",
    "# Print classification reports\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(f\"Classification Report\")\n",
    "report = classification_report(y_test, test_pred, zero_division=1)\n",
    "print(report)\n",
    "print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a7928",
   "metadata": {},
   "source": [
    "## Under Sampled Scaled Data with Untuned RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd2c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomUnderSampler from imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Instantiate a RandomOversampler instance\n",
    "rus = RandomUnderSampler(random_state=1)\n",
    "\n",
    "# Fit the training data to the `RandomOverSampler` model\n",
    "X_undersampled, y_undersampled = rus.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Count distinct values\n",
    "y_undersampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a6d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "rus_clf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Fit the model\n",
    "rus_clf.fit(X_undersampled, y_undersampled)\n",
    "\n",
    "# Predict using the model\n",
    "train_pred = rus_clf.predict(X_train_scaled)\n",
    "test_pred = rus_clf.predict(X_test_scaled)\n",
    "\n",
    "# Generate classification report\n",
    "# Print scores\n",
    "print(\"Balanced Accuracy Scores\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "x = balanced_accuracy_score(y_train, train_pred)\n",
    "y = balanced_accuracy_score(y_test, test_pred)\n",
    "print(balanced_accuracy_score(y_train, train_pred),'training')\n",
    "print(balanced_accuracy_score(y_test, test_pred),'testing')\n",
    "print(round((x-y), 16),'variance')\n",
    "\n",
    "# Print classification reports\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(f\"Classification Report\")\n",
    "report = classification_report(y_test, test_pred, zero_division=1)\n",
    "print(report)\n",
    "print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f35e31",
   "metadata": {},
   "source": [
    "## Original Scaled Data and Untuned XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f27409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "xgb_clf = XGBClassifier(random_state=1)\n",
    "\n",
    "# Fit the model\n",
    "xgb_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict using the model\n",
    "train_pred = xgb_clf.predict(X_train_scaled)\n",
    "test_pred = xgb_clf.predict(X_test_scaled)\n",
    "\n",
    "# Plot feature importance\n",
    "from xgboost import plot_importance\n",
    "plot_importance(xgb_clf)\n",
    "plt.show()\n",
    "\n",
    "# Generate classification report\n",
    "# Print scores\n",
    "print(\"Balanced Accuracy Scores\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "x = balanced_accuracy_score(y_train, train_pred)\n",
    "y = balanced_accuracy_score(y_test, test_pred)\n",
    "print(balanced_accuracy_score(y_train, train_pred),'training')\n",
    "print(balanced_accuracy_score(y_test, test_pred),'testing')\n",
    "print(round((x-y), 16),'variance')\n",
    "\n",
    "# Print classification reports\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(f\"Classification Report\")\n",
    "report = classification_report(y_test, test_pred, zero_division=1)\n",
    "print(report)\n",
    "print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913ab0d5",
   "metadata": {},
   "source": [
    "## Under Sampled Scaled Data and Untuned XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "xgb_rus_clf = XGBClassifier(random_state=1)\n",
    "\n",
    "# Fit the model\n",
    "xgb_rus_clf.fit(X_undersampled, y_undersampled)\n",
    "\n",
    "# Predict using the model\n",
    "train_pred = xgb_rus_clf.predict(X_train_scaled)\n",
    "test_pred = xgb_rus_clf.predict(X_test_scaled)\n",
    "\n",
    "# Plot feature importance\n",
    "plot_importance(xgb_rus_clf)\n",
    "plt.show()\n",
    "\n",
    "# Generate classification report\n",
    "# Print scores\n",
    "print(\"Balanced Accuracy Scores\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "x = balanced_accuracy_score(y_train, train_pred)\n",
    "y = balanced_accuracy_score(y_test, test_pred)\n",
    "print(balanced_accuracy_score(y_train, train_pred),'training')\n",
    "print(balanced_accuracy_score(y_test, test_pred),'testing')\n",
    "print(round((x-y), 16),'variance')\n",
    "\n",
    "# Print classification reports\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(f\"Classification Report\")\n",
    "report = classification_report(y_test, test_pred, zero_division=1)\n",
    "print(report)\n",
    "print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadb0b7",
   "metadata": {},
   "source": [
    "## Under Sampled Scaled Data with XGBoost Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92283eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the following values for learning_rate\n",
    "rate = np.arange(0.01, 1.1, 0.01)\n",
    "models = {'train_score': [], 'test_score': [], 'learning_rate': []}\n",
    "\n",
    "# Loop through each value in learning_rates\n",
    "for r in rate:\n",
    "    # Initialize the classifier with parameter variables\n",
    "    xgb_rus_clf = XGBClassifier(learning_rate = r, tree_method='approx', random_state=1)\n",
    "\n",
    "    # Fit the undersampled data the new model\n",
    "    xgb_rus_clf.fit(X_undersampled, y_undersampled)\n",
    "\n",
    "    # Make predictions\n",
    "    train_pred = xgb_rus_clf.predict(X_train_scaled)\n",
    "    test_pred = xgb_rus_clf.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate balanced accuracy scores\n",
    "    train_score = balanced_accuracy_score(y_train, train_pred)\n",
    "    test_score = balanced_accuracy_score(y_test, test_pred)\n",
    "\n",
    "    # Append scores\n",
    "    models['train_score'].append(train_score)\n",
    "    models['test_score'].append(test_score)\n",
    "    models['learning_rate'].append(r)\n",
    "\n",
    "# Create a dataframe from the models dictionary with learning_rate as the index\n",
    "models_df = pd.DataFrame(models).set_index('learning_rate')\n",
    "\n",
    "# Display df\n",
    "display(models_df.sort_values(by='test_score', ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0014b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by test_score in descending order and get the top 3\n",
    "top_learning_rates = models_df.sort_values(by='test_score', ascending=False).head(3).index.tolist()\n",
    "\n",
    "# Display the top 3 \n",
    "top_learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1744aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "models_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f531656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the following values for n_estimators\n",
    "estimators = range(100, 200, 1)\n",
    "models = {'train_score': [], 'test_score': [], 'n_estimators': []}\n",
    "\n",
    "# Loop through each value in n_estimators\n",
    "for n in estimators:\n",
    "    # Initialize the classifier with parameter variables\n",
    "    xgb_rus_clf = XGBClassifier(n_estimators = n, tree_method='approx', random_state=1)\n",
    "\n",
    "    # Fit the undersampled data the new model\n",
    "    xgb_rus_clf.fit(X_undersampled, y_undersampled)\n",
    "\n",
    "    # Make predictions\n",
    "    train_pred = xgb_rus_clf.predict(X_train_scaled)\n",
    "    test_pred = xgb_rus_clf.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate balanced accuracy scores\n",
    "    train_score = balanced_accuracy_score(y_train, train_pred)\n",
    "    test_score = balanced_accuracy_score(y_test, test_pred)\n",
    "\n",
    "    # Append scores\n",
    "    models['train_score'].append(train_score)\n",
    "    models['test_score'].append(test_score)\n",
    "    models['n_estimators'].append(n)\n",
    "\n",
    "# Create a dataframe from the models dictionary with n_estimators as the index\n",
    "models_df = pd.DataFrame(models).set_index('n_estimators')\n",
    "\n",
    "# Display df\n",
    "display(models_df.sort_values(by='test_score', ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82eb4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by test_score in descending order and get the top 3\n",
    "top_n_estimators = models_df.sort_values(by='test_score', ascending=False).head(3).index.tolist()\n",
    "\n",
    "# Display the top 3\n",
    "top_n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f41cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "models_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e47164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the following values for max_depth\n",
    "max_depths = range(1, 30)\n",
    "models = {'train_score': [], 'test_score': [], 'max_depth': []}\n",
    "\n",
    "# Loop through each value in max_depths\n",
    "for depth in max_depths:\n",
    "    # Initialize the classifier with parameter variables\n",
    "    xgb_rus_clf = XGBClassifier(max_depth = depth, tree_method='approx', random_state=1)\n",
    "    \n",
    "    # Fit the undersampled data the new model\n",
    "    xgb_rus_clf.fit(X_undersampled, y_undersampled)\n",
    "\n",
    "    # Make predictions\n",
    "    train_pred = xgb_rus_clf.predict(X_train_scaled)\n",
    "    test_pred = xgb_rus_clf.predict(X_test_scaled)\n",
    "\n",
    "    # Caclulate balanced accuracy scores\n",
    "    train_score = balanced_accuracy_score(y_train, train_pred)\n",
    "    test_score = balanced_accuracy_score(y_test, test_pred)\n",
    "\n",
    "    # Append scores\n",
    "    models['train_score'].append(train_score)\n",
    "    models['test_score'].append(test_score)\n",
    "    models['max_depth'].append(depth)\n",
    "\n",
    "# Create a dataframe from the models dictionary with max_depth as the index\n",
    "models_df = pd.DataFrame(models).set_index('max_depth')\n",
    "\n",
    "# Display df\n",
    "display(models_df.sort_values(by='test_score', ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by test_score in descending order and get the top 3\n",
    "top_max_depth = models_df.sort_values(by='test_score', ascending=False).head(3).index.tolist()\n",
    "\n",
    "# Display the top 3\n",
    "top_max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19270c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "models_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5865aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the following values for max_leaves\n",
    "max_leaves = range(1, 30)\n",
    "models = {'train_score': [], 'test_score': [], 'max_leaves': []}\n",
    "\n",
    "# Loop through each value in max_depths\n",
    "for leaf in max_leaves:\n",
    "    # Initialize the classifier with parameter variables\n",
    "    xgb_rus_clf = XGBClassifier(max_leaves = leaf, tree_method='approx', random_state=1)\n",
    "    \n",
    "    # Fit the undersampled data the new model\n",
    "    xgb_rus_clf.fit(X_undersampled, y_undersampled)\n",
    "\n",
    "    # Make predictions\n",
    "    train_pred = xgb_rus_clf.predict(X_train_scaled)\n",
    "    test_pred = xgb_rus_clf.predict(X_test_scaled)\n",
    "\n",
    "    # Caclulate balanced accuracy scores\n",
    "    train_score = balanced_accuracy_score(y_train, train_pred)\n",
    "    test_score = balanced_accuracy_score(y_test, test_pred)\n",
    "\n",
    "    # Append scores\n",
    "    models['train_score'].append(train_score)\n",
    "    models['test_score'].append(test_score)\n",
    "    models['max_leaves'].append(leaf)\n",
    "\n",
    "# Create a dataframe from the models dictionary with max_depth as the index\n",
    "models_df = pd.DataFrame(models).set_index('max_leaves')\n",
    "\n",
    "# Display df\n",
    "display(models_df.sort_values(by='test_score', ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa969c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by test_score in descending order and get the top 3\n",
    "top_max_leaves = models_df.sort_values(by='test_score', ascending=False).head(3).index.tolist()\n",
    "\n",
    "# Display the top 3\n",
    "top_max_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868df399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "models_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55964a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the following values for subsample\n",
    "subsample = np.arange(0, 1, .01)\n",
    "models = {'train_score': [], 'test_score': [], 'subsample': []}\n",
    "\n",
    "# Loop through each value in max_depths\n",
    "for sample in subsample:\n",
    "    # Initialize the classifier with parameter variables\n",
    "    xgb_rus_clf = XGBClassifier(subsample = sample, tree_method='approx', random_state=1)\n",
    "    \n",
    "    # Fit the undersampled data the new model\n",
    "    xgb_rus_clf.fit(X_undersampled, y_undersampled)\n",
    "\n",
    "    # Make predictions\n",
    "    train_pred = xgb_rus_clf.predict(X_train_scaled)\n",
    "    test_pred = xgb_rus_clf.predict(X_test_scaled)\n",
    "\n",
    "    # Caclulate balanced accuracy scores\n",
    "    train_score = balanced_accuracy_score(y_train, train_pred)\n",
    "    test_score = balanced_accuracy_score(y_test, test_pred)\n",
    "\n",
    "    # Append scores\n",
    "    models['train_score'].append(train_score)\n",
    "    models['test_score'].append(test_score)\n",
    "    models['subsample'].append(sample)\n",
    "\n",
    "# Create a dataframe from the models dictionary with max_depth as the index\n",
    "models_df = pd.DataFrame(models).set_index('subsample')\n",
    "\n",
    "# Display df\n",
    "display(models_df.sort_values(by='test_score', ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1b630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by test_score in descending order and get the top 3\n",
    "top_subsample = models_df.sort_values(by='test_score', ascending=False).head(3).index.tolist()\n",
    "\n",
    "# Display the top 3\n",
    "top_subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89deaed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "models_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a21390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with 3 values for each parameter\n",
    "params = {\n",
    "    'n_estimators': top_n_estimators,\n",
    "    'max_depth': top_max_depth,\n",
    "    'learning_rate': top_learning_rates,\n",
    "    'max_leaves': top_max_leaves,\n",
    "    'subsample': top_subsample\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {'n_estimators': [], 'max_depth': [], 'learning_rate': [],'max_leaves': [], 'subsample': [], 'train_score': [], 'test_score': []}\n",
    "\n",
    "# Loop through all combinations of parameters\n",
    "for n in params['n_estimators']:\n",
    "    for d in params['max_depth']:\n",
    "        for lr in params['learning_rate']:\n",
    "            for ml in params['max_leaves']:\n",
    "                for s in params['subsample']:\n",
    "                    # Initialize the classifier with current parameters\n",
    "                    xgb_rus_clf = XGBClassifier(n_estimators=n, learning_rate=lr, max_depth=d, max_leaves=ml, subsample=s, tree_method='approx', random_state=1)\n",
    "                \n",
    "                    # Fit the undersampled data\n",
    "                    xgb_rus_clf.fit(X_undersampled, y_undersampled)\n",
    "    \n",
    "                    # Make predictions\n",
    "                    train_pred = xgb_rus_clf.predict(X_train_scaled)\n",
    "                    test_pred = xgb_rus_clf.predict(X_test_scaled)\n",
    "    \n",
    "                    # Calculate balanced accuracy scores\n",
    "                    train_score = balanced_accuracy_score(y_train, train_pred)\n",
    "                    test_score = balanced_accuracy_score(y_test, test_pred)\n",
    "    \n",
    "                    # Append results to the dictionary\n",
    "                    results['n_estimators'].append(n)\n",
    "                    results['max_depth'].append(d)\n",
    "                    results['learning_rate'].append(lr)\n",
    "                    results['max_leaves'].append(ml)\n",
    "                    results['subsample'].append(s)\n",
    "                    results['train_score'].append(train_score)\n",
    "                    results['test_score'].append(test_score)\n",
    "\n",
    "# Create a DataFrame from the results dictionary\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb6e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results_df = results_df.sort_values(by='test_score', ascending=False)\n",
    "print(sorted_results_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029224f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Classification\n",
    "# Initialize the classifier with current parameters\n",
    "xgb_rus_clf = XGBClassifier(n_estimators=162, max_depth=15, learning_rate=.02, max_leaves=29, subsample=.43, tree_method='approx', min_split_loss=0, min_child_weight=1, random_state=1)\n",
    "\n",
    "# Fit the undersampled data\n",
    "xgb_rus_clf.fit(X_undersampled, y_undersampled)\n",
    "\n",
    "# Make predictions\n",
    "train_pred = xgb_rus_clf.predict(X_train_scaled)\n",
    "test_pred = xgb_rus_clf.predict(X_test_scaled)\n",
    "\n",
    "# Generate classification report\n",
    "# Print scores\n",
    "print(\"Balanced Accuracy Scores\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "x = balanced_accuracy_score(y_train, train_pred)\n",
    "y = balanced_accuracy_score(y_test, test_pred)\n",
    "print(balanced_accuracy_score(y_train, train_pred),'training score')\n",
    "print(balanced_accuracy_score(y_test, test_pred),'testing score')\n",
    "print(round((x-y), 16),'variance')\n",
    "\n",
    "# Print classification reports\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(f\"Classification Report - Original Data\")\n",
    "print(classification_report(y_test, test_pred))\n",
    "print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7983b0",
   "metadata": {},
   "source": [
    "## Original Scaled Data and Untuned LightBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89263547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "lgb_clf = lgb.LGBMClassifier(random_state=1, verbose=-1)\n",
    "\n",
    "# Fit the model\n",
    "lgb_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict using the model\n",
    "train_pred = lgb_clf.predict(X_train_scaled)\n",
    "test_pred = lgb_clf.predict(X_test_scaled)\n",
    "\n",
    "# Generate classification report\n",
    "# Print scores\n",
    "print(\"Balanced Accuracy Scores\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "x = balanced_accuracy_score(y_train, train_pred)\n",
    "y = balanced_accuracy_score(y_test, test_pred)\n",
    "print(balanced_accuracy_score(y_train, train_pred),'training')\n",
    "print(balanced_accuracy_score(y_test, test_pred),'testing')\n",
    "print(round((x-y), 16),'variance')\n",
    "\n",
    "# Print classification reports\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(f\"Classification Report\")\n",
    "report = classification_report(y_test, test_pred, zero_division=1)\n",
    "print(report)\n",
    "print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da40aca",
   "metadata": {},
   "source": [
    "## Under Sampled Scaled Data with Untuned LightBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc613475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "lgb_rus_clf = lgb.LGBMClassifier(random_state=1, verbose=-1)\n",
    "\n",
    "# Fit the model\n",
    "lgb_rus_clf.fit(X_undersampled, y_undersampled)\n",
    "\n",
    "# Predict using the model\n",
    "train_pred = lgb_rus_clf.predict(X_train_scaled)\n",
    "test_pred = lgb_rus_clf.predict(X_test_scaled)\n",
    "\n",
    "# Generate classification report\n",
    "# Print scores\n",
    "print(\"Balanced Accuracy Scores\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "x = balanced_accuracy_score(y_train, train_pred)\n",
    "y = balanced_accuracy_score(y_test, test_pred)\n",
    "print(balanced_accuracy_score(y_train, train_pred),'training')\n",
    "print(balanced_accuracy_score(y_test, test_pred),'testing')\n",
    "print(round((x-y), 16),'variance')\n",
    "\n",
    "# Print classification reports\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(f\"Classification Report\")\n",
    "report = classification_report(y_test, test_pred, zero_division=1)\n",
    "print(report)\n",
    "print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a310bced",
   "metadata": {},
   "source": [
    "## RandomUnderSampler Data with Tuned Lightboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47550160",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = np.arange(0.05, 1.0, 0.01)\n",
    "models = {'train_score': [], 'test_score': [], 'learning_rate': []}\n",
    "\n",
    "# Loop through each value in rates\n",
    "for r in rates:\n",
    "    # Initialize the classifier with current parameters\n",
    "    lgb_rus_clf = lgb.LGBMClassifier(learning_rate=r, random_state=1, verbose=-1)\n",
    "\n",
    "    # Fit the undersampled data\n",
    "    lgb_rus_clf.fit(X_undersampled, y_undersampled)\n",
    "\n",
    "    # Make predictions\n",
    "    train_pred = lgb_rus_clf.predict(X_train)\n",
    "    test_pred = lgb_rus_clf.predict(X_test)\n",
    "\n",
    "    # Calculate balanced accuracy score\n",
    "    train_score = balanced_accuracy_score(y_train, train_pred)\n",
    "    test_score = balanced_accuracy_score(y_test, test_pred)\n",
    "\n",
    "    # Append scores\n",
    "    models['train_score'].append(train_score)\n",
    "    models['test_score'].append(test_score)\n",
    "    models['learning_rate'].append(r)\n",
    "\n",
    "# Create a dataframe from the models dictionary with learning_rate as the index\n",
    "models_df = pd.DataFrame(models).set_index('learning_rate')\n",
    "\n",
    "# Display df\n",
    "display(models_df.sort_values(by='test_score', ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d9643",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = np.arange(0.05, 1.0, 0.01)\n",
    "models = {'train_score': [], 'test_score': [], 'learning_rate': []}\n",
    "\n",
    "# Loop through each value in rates\n",
    "for r in rates:\n",
    "    # Initialize the classifier with current parameters\n",
    "    lgb_rus_clf = lgb.LGBMClassifier(learning_rate=r, random_state=1, verbose=-1)\n",
    "\n",
    "    # Fit the undersampled data\n",
    "    lgb_rus_clf.fit(X_undersampled, y_undersampled)\n",
    "\n",
    "    # Make predictions\n",
    "    train_pred = lgb_rus_clf.predict(X_train)\n",
    "    test_pred = lgb_rus_clf.predict(X_test)\n",
    "\n",
    "    # Calculate balanced accuracy score\n",
    "    train_score = balanced_accuracy_score(y_train, train_pred)\n",
    "    test_score = balanced_accuracy_score(y_test, test_pred)\n",
    "\n",
    "    # Append scores\n",
    "    models['train_score'].append(train_score)\n",
    "    models['test_score'].append(test_score)\n",
    "    models['learning_rate'].append(r)\n",
    "\n",
    "# Create a dataframe from the models dictionary with learning_rate as the index\n",
    "models_df = pd.DataFrame(models).set_index('learning_rate')\n",
    "\n",
    "# Display df\n",
    "display(models_df.sort_values(by='test_score', ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8989cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "models_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ee26c-90ad-4f98-b600-9ccc20743e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the following values for n_estimators\n",
    "estimators = range(100, 250, 1)\n",
    "models = {'train_score': [], 'test_score': [], 'n_estimators': []}\n",
    "\n",
    "# Loop through each value in n_estimators\n",
    "for n in estimators:\n",
    "    # Initialize the classifier with current parameters\n",
    "    lgb_rus_clf =lgb.LGBMClassifier(n_estimators = n, random_state=1, verbose=-1)\n",
    "\n",
    "    # Fit the undersampled data\n",
    "    lgb_rus_clf.fit(X_undersampled, y_undersampled)\n",
    "\n",
    "    # Make predictions\n",
    "    train_pred = lgb_rus_clf.predict(X_train)\n",
    "    test_pred = lgb_rus_clf.predict(X_test)\n",
    "\n",
    "    # Calculate balanced accuracy scores\n",
    "    train_score = balanced_accuracy_score(y_train, train_pred)\n",
    "    test_score = balanced_accuracy_score(y_test, test_pred)\n",
    "\n",
    "    # Append scores\n",
    "    models['train_score'].append(train_score)\n",
    "    models['test_score'].append(test_score)\n",
    "    models['n_estimators'].append(n)\n",
    "\n",
    "# Create a dataframe from the models dictionary with n_estimators as the index\n",
    "models_df = pd.DataFrame(models).set_index('n_estimators')\n",
    "\n",
    "# Display df\n",
    "display(models_df.sort_values(by='test_score', ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a889e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by test_score in descending order and get the top 3\n",
    "n_estimators = models_df.sort_values(by='test_score', ascending=False).head(3).index.tolist()\n",
    "\n",
    "# Display the top 3\n",
    "n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93031a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "models_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a78dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the following values for max_depth\n",
    "max_depths = range(2, 30)\n",
    "models = {'train_score': [], 'test_score': [], 'max_depth': []}\n",
    "\n",
    "# Loop through each value in max_depths\n",
    "for depth in max_depths:\n",
    "    # Initialize the classifier with current parameters\n",
    "    lgb_rus_clf = lgb.LGBMClassifier(max_depth = depth, random_state=1, verbose=-1)\n",
    "    \n",
    "    # Fit the undersampled data\n",
    "    lgb_rus_clf.fit(X_undersampled, y_undersampled)\n",
    "\n",
    "    # Make predictions\n",
    "    train_pred = lgb_rus_clf.predict(X_train)\n",
    "    test_pred = lgb_rus_clf.predict(X_test)\n",
    "\n",
    "    # Calculate balanced accuracy scores\n",
    "    train_score = balanced_accuracy_score(y_train, train_pred)\n",
    "    test_score = balanced_accuracy_score(y_test, test_pred)\n",
    " \n",
    "    # Append scores\n",
    "    models['train_score'].append(train_score)\n",
    "    models['test_score'].append(test_score)\n",
    "    models['max_depth'].append(depth)\n",
    "\n",
    "# Create a dataframe from the models dictionary with max_depth as the index\n",
    "models_df = pd.DataFrame(models).set_index('max_depth')\n",
    "\n",
    "# Display df\n",
    "display(models_df.sort_values(by='test_score', ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by test_score in descending order and get the top 3\n",
    "max_depth = models_df.sort_values(by='test_score', ascending=False).head(3).index.tolist()\n",
    "\n",
    "# Display the top 3 \n",
    "max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "models_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d82a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the following values for num_leaves\n",
    "num_leaves = range(10, 50)\n",
    "models = {'train_score': [], 'test_score': [], 'num_leaves': []}\n",
    "\n",
    "# Loop through each value in num_leaves\n",
    "for num in num_leaves:\n",
    "    # Initialize the classifier with current parameters\n",
    "    lgb_rus_clf = lgb.LGBMClassifier(num_leaves = num, random_state=1, verbose=-1)\n",
    "    \n",
    "    # Fit the undersampled data\n",
    "    lgb_rus_clf.fit(X_undersampled, y_undersampled)\n",
    "\n",
    "    # Make predictions\n",
    "    train_pred = lgb_rus_clf.predict(X_train)\n",
    "    test_pred = lgb_rus_clf.predict(X_test)\n",
    "\n",
    "    # Calculate balanced accuracy scores\n",
    "    train_score = balanced_accuracy_score(y_train, train_pred)\n",
    "    test_score = balanced_accuracy_score(y_test, test_pred)\n",
    " \n",
    "    # Append scores\n",
    "    models['train_score'].append(train_score)\n",
    "    models['test_score'].append(test_score)\n",
    "    models['num_leaves'].append(num)\n",
    "\n",
    "# Create a dataframe from the models dictionary with max_depth as the index\n",
    "models_df = pd.DataFrame(models).set_index('num_leaves')\n",
    "\n",
    "# Display df\n",
    "display(models_df.sort_values(by='test_score', ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de172a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by test_score in descending order and get the top 3\n",
    "num_leaves = models_df.sort_values(by='test_score', ascending=False).head(3).index.tolist()\n",
    "\n",
    "# Display the top 3 \n",
    "num_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca94b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "models_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the following values for max_bin\n",
    "max_bin = range(20, 512, 1)\n",
    "models = {'train_score': [], 'test_score': [], 'max_bin': []}\n",
    "\n",
    "# Loop through each value in max_bins\n",
    "for bin in max_bin:\n",
    "    # Initialize the classifier with current parameters\n",
    "    lgb_rus_clf = lgb.LGBMClassifier(max_bin = bin, random_state=1, verbose=-1)\n",
    "    \n",
    "    # Fit the undersampled data\n",
    "    lgb_rus_clf.fit(X_undersampled, y_undersampled)\n",
    "\n",
    "    # Make predictions\n",
    "    train_pred = lgb_rus_clf.predict(X_train)\n",
    "    test_pred = lgb_rus_clf.predict(X_test)\n",
    "\n",
    "    # Calculate balanced accuracy scores\n",
    "    train_score = balanced_accuracy_score(y_train, train_pred)\n",
    "    test_score = balanced_accuracy_score(y_test, test_pred)\n",
    " \n",
    "    # Append scores\n",
    "    models['train_score'].append(train_score)\n",
    "    models['test_score'].append(test_score)\n",
    "    models['max_bin'].append(bin)\n",
    "\n",
    "# Create a dataframe from the models dictionary with max_depth as the index\n",
    "models_df = pd.DataFrame(models).set_index('max_bin')\n",
    "\n",
    "# Display df\n",
    "display(models_df.sort_values(by='test_score', ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cdab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by test_score in descending order and get the top 3\n",
    "max_bin = models_df.sort_values(by='test_score', ascending=False).head(3).index.tolist()\n",
    "\n",
    "# Display the top 3 \n",
    "max_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7e7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "models_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e4382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with 3 values for each parameter\n",
    "params = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth': max_depth,\n",
    "    'learning_rate': learning_rate,\n",
    "    'num_leaves': num_leaves,\n",
    "    'max_bin': max_bin\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {'n_estimators': [], 'max_depth': [], 'learning_rate': [], 'num_leaves': [], 'max_bin': [], 'train_score': [], 'test_score': []}\n",
    "\n",
    "# Loop through all combinations of parameters\n",
    "for n in params['n_estimators']:\n",
    "    for d in params['max_depth']:\n",
    "        for lr in params['learning_rate']:\n",
    "            for nl in params['num_leaves']:\n",
    "                for b in params['max_bin']:\n",
    "                    # Initialize the classifier with current parameters\n",
    "                    lgb_rus_clf = lgb.LGBMClassifier(n_estimators=n, learning_rate=lr, max_depth=d, num_leaves=nl, max_bin=b, random_state=1, verbose=-1)\n",
    "                \n",
    "                    # Fit the undersampled data\n",
    "                    lgb_rus_clf.fit(X_undersampled, y_undersampled)\n",
    "    \n",
    "                    # Make predictions\n",
    "                    train_pred = lgb_rus_clf.predict(X_train)\n",
    "                    test_pred = lgb_rus_clf.predict(X_test)\n",
    "    \n",
    "                    # Calculate balanced accuracy scores\n",
    "                    train_score = balanced_accuracy_score(y_train, train_pred)\n",
    "                    test_score = balanced_accuracy_score(y_test, test_pred)\n",
    "    \n",
    "                    # Append results to the dictionary\n",
    "                    results['n_estimators'].append(n)\n",
    "                    results['max_depth'].append(d)\n",
    "                    results['learning_rate'].append(lr)\n",
    "                    results['num_leaves'].append(nl)\n",
    "                    results['max_bin'].append(b)\n",
    "                    results['train_score'].append(train_score)\n",
    "                    results['test_score'].append(test_score)\n",
    "                    \n",
    "# Create a DataFrame from the results dictionary\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaeba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results_df = results_df.sort_values(by='test_score', ascending=False)\n",
    "print(sorted_results_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c52ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Classification\n",
    "# Initialize the classifier with current parameters\n",
    "lgb_rus_clf = lgb.LGBMClassifier(n_estimators=105, max_depth=16, learning_rate=.06, num_leaves=22, max_bin=67, random_state=1, verbose=-1)\n",
    "\n",
    "# Fit the undersampled data\n",
    "lgb_rus_clf.fit(X_undersampled, y_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "train_pred = lgb_rus_clf.predict(X_train)\n",
    "test_pred = lgb_rus_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca8a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model's balanced accuracy on the training and test sets\n",
    "print('\\nRandomUnderSampled Data and Tuned LGB Classifer')\n",
    "\n",
    "# Print scores\n",
    "print(\"--------------------------------------------------------\")\n",
    "x = balanced_accuracy_score(y_train, train_pred)\n",
    "y = balanced_accuracy_score(y_test, test_pred)\n",
    "print(balanced_accuracy_score(y_train, train_pred),'training score')\n",
    "print(balanced_accuracy_score(y_test, test_pred),'testing score')\n",
    "print(round((x-y), 16),'variance')\n",
    "\n",
    "# Print classification reports\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(f\"Classification Report - Original Data\")\n",
    "print(classification_report(y_test, test_pred))\n",
    "print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importance array\n",
    "importances = lgb_rus_clf.feature_importances_\n",
    "# List the top 10 most important features\n",
    "importances_sorted = sorted(zip(lgb_rus_clf.feature_importances_, X.columns), reverse=True)\n",
    "importances_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
